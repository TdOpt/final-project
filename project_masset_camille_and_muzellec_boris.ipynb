{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Course Optimization for Data Science\n",
    "## Optimization strategies for Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Alexandre Gramfort, Stéphane Gaiffas\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- derive the duals for SVMs with and without intercept\n",
    "- implement an SVM using a blackbox convex toolbox (cvxopt in Python)\n",
    "- implement your own solvers for the without intercept case: Proximal gradient, Coordinate Descent, Newton, Quasi-Newton\n",
    "- Present a clear benchmark of the different strategies on small and medium scale datasets\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 3rd of January at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"camille\"\n",
    "ln1 = \"masset\"\n",
    "fn2 = \"boris\"\n",
    "ln2 = \"muzellec\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important:\n",
    "\n",
    "For Part 0 to Part 2 of the project you will need a working install of `cvxopt`.\n",
    "You may struggle a bit to set it up.\n",
    "The simplest way of getting it is by typing \n",
    "\n",
    "`pip install cvxopt`\n",
    "\n",
    "if you have `pip` installed on your laptop.\n",
    "If you **struggle too much please\n",
    "contact us**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: SVM Classification with linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the problem of binary classification from $n$ observations\n",
    "$x_i \\in \\mathbb{R}^{d}$,\n",
    "$1 \\leq i \\leq n$. We aim to learn a function:\n",
    "$$f: x \\in \\mathbb{R}^{d}\\mapsto y\\in\\{-1,+1\\}$$\n",
    "from the $n$ annotated training samples $(x_{i},y_{i})$ supposed i.i.d. from an unknown probability distribution on $\\mathbb{R}^d \\times \\{-1,+1\\}$. Once this function is learnt, it will be possible to use it to predict the label $y$ associated to a new sample $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin and linear separating hyperplane:\n",
    "\n",
    "<img src=\"separateur.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear case, one looks for an affine function of $x$ of the form \n",
    "$f(x) = \\mathrm{sign}(w^{\\top} x)$ or $f(x)=\\mathrm{sign}(w^{\\top}x + b)$\n",
    "with $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$. The first case is referred\n",
    "to as the **without intercept** case. Indeed the coefficient $b$ is known\n",
    "as the intercept or bias term.\n",
    "\n",
    "We will start by considering the case with intercept.\n",
    "\n",
    "To learn $f$, we use the $n$ annotated samples and one looks for a hyperplane $P(w,b)$\n",
    "such that the smallest distance to positive and negative samples\n",
    "is the largest. This can be written as:\n",
    "$$\n",
    " \\max_{w,b} \\min_{i=1:n} y_i \\delta(x_{i},P(w,b)) \\quad\n",
    " \\text{where}\\quad d(x_{i},P(w,b)) = \\frac{|w^{\\top}x_{i}+b|}{\\sqrt{w^{\\top}w}} \\enspace,\n",
    "$$\n",
    "since the signed distance from a sample $x_{i}$ to the hyperplane $P(w,b)$ is given by\n",
    "$$\n",
    " \\delta(x_{i},w,b) = \\frac{w^{\\top}x_{i}+b}{\\sqrt{w^{\\top}w}}.\n",
    "$$\n",
    "The principle described above is the maximisation of the *margin*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that if the minimum of a set of values is larger than $m$ than all values of the set are larger than $m$. This leads to the following problem formulation:\n",
    "$$\n",
    " \\left\\{\n",
    " \\begin{array}{cll}\n",
    " \\max_{(w,b)} \\quad m \\\\\n",
    " \\text{s.t.} \\;\\; &\\forall i &y_i\\dfrac{w^{\\top}x_{i}+b}{\\sqrt{w^{\\top}w}}\\geq m\n",
    " \\end{array}\n",
    " \\right. \\enspace .\n",
    "$$\n",
    "\n",
    "The hyperplane separates the space in 2 half spaces, depending if $\\delta(x_{i},w,b)$ is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all samples are linearly separable, convince yourself that the problem can be written as:\n",
    "$$\n",
    "(\\mathcal{P}):  \\left\\{\n",
    " \\begin{array}{cll}\n",
    " &\\min_{(w,b)} \\frac{1}{2}w^{\\top}w\n",
    " \\\\\n",
    "  &y_{i}(w^{\\top}x_{i}+b)\\geq 1, \\quad \\forall i\\in \\{1,\\cdots,n\\}\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q1: Justify that the problem $(\\mathcal{P})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The objective function is quadratic ($\\frac{1}{2}w^\\top w = \\frac{1}{2}\\Vert w \\Vert^2$) and thus convex, the conditions are linear and thus convex too, therefore problem $(\\mathcal{P})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q2: By looking at the saddle points of the Lagrangian $\\mathcal{L}(w, b, \\mu)$, $\\mu \\in \\mathbb{R}_+^n$, show that the dual problem $(\\mathcal{D})$ can be written as:\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}): \n",
    " \\left\\{\n",
    " \\begin{array}{lll}\n",
    " \\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{t}\\mu-\\mu^{\\top}u\n",
    " \\\\\n",
    " \\mathrm{s.c.}& y^{\\top}\\mu = 0\n",
    " \\\\\n",
    " \\mathrm{and}& -\\mu \\leq  0\n",
    " \\end{array}\n",
    " \\right .\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    " G = \\begin{bmatrix}y_{1}x_{1}^{\\top} \\\\ \\vdots \\\\ y_{n}x_{n}^{\\top}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $u = (1, \\dots, 1) \\in \\mathbb{R}^n$.\n",
    "\n",
    "We will **assume here qualification of the contraints**.\n",
    "\n",
    "Remark: The problem $(\\mathcal{D})$ is a *quadratic program* (QP) for which their exist off-the-shelf techniques. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The Lagrangian of the problem is:\n",
    "$$\\mathcal{L}(w, b, \\mu) = \\frac{1}{2} \\Vert w \\Vert^2 - \\sum_{i=1}^n \\mu_i [y_i(w^\\top x_i + b) - 1]$$\n",
    "Taking the derivative at a saddle point $(w, b, \\mu)$, we have:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^n \\mu_i y_i x_i = 0 \\quad \\text{and} \\quad \\frac{\\partial \\mathcal{L}}{\\partial b} = - \\sum_{i=1}^n \\mu_i y_i = 0$$\n",
    "\n",
    "Now we introduce the matrix $G = (y_i x_i^\\top)_{1 \\leq i \\leq n} \\in \\mathcal{M}_{n,d}(\\mathbb{R})$. We notice that:\n",
    "$$G^\\top \\mu = \\sum_{i=1}^n \\mu_i y_i x_i = w \\quad \\text{at a saddle point of } \\mathcal{L}$$\n",
    "\n",
    "We subtitute these values in the general expression of the Lagragian, at a saddle point:\n",
    "$$\\mathcal{L}(w, b, \\mu) = \\frac{1}{2} \\left( G^\\top \\mu \\right)^\\top \\left( G^\\top \\mu \\right) + \\sum_{i=1}^n \\mu_i - b \\sum_{i=1}^n \\mu_i y_i - \\sum_{i=1}^n \\mu_i y_i (G^\\top \\mu)^\\top x_i$$\n",
    "\n",
    "Therefore, with $u = (1, \\dots, 1)^\\top \\in \\mathbb{R}^n$:\n",
    "$$\\mathcal{L}(w, b, \\mu) = \\frac{1}{2} \\left( G^\\top \\mu \\right)^\\top \\left( G^\\top \\mu \\right) + \\mu^\\top u - (G^\\top \\mu)^\\top (G^\\top \\mu)$$\n",
    "$$\\mathcal{L}(w, b, \\mu) = \\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu^\\top u - \\mu^\\top G G^\\top \\mu$$\n",
    "Finally, at a saddle point, we have:\n",
    "$$\\mathcal{L}(w, b, \\mu) = - \\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu^\\top u \\quad \\text{ and } \\quad \\mu^\\top y = 0$$\n",
    "\n",
    "Considering the optimisation problem $(\\mathcal{P})$, which is convex with affine inequalities constraints (that are supposed qualified), we can apply the Kuhn and Tucker theorem:\n",
    "$$(w, b) \\text{ is a minimiser of } J(w,b) = \\frac{1}{2}w^\\top w \\text{ with the constraints } -y_i(w^\\top x_i+b) + 1 \\leq 0$$\n",
    "$$\\implies \\exists \\mu \\in \\mathbb{R}^n \\text{ s.t. } \\mu \\geq 0 \\text{ and } ((w, b), \\mu) \\text{ is a saddle point of the Lagragian } \\mathcal{L}$$\n",
    "\n",
    "Therefore, we can consider the dual problem:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\max_\\mu \\left\\lbrace -\\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } \\mu \\geq 0 \\\\ \\text{and } y^\\top \\mu = 0 \\end{cases}$$\n",
    "which is equivalent to:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\min_\\mu \\left\\lbrace \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } -\\mu \\leq 0 \\\\ \\text{and } y^\\top \\mu = 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q3: Justify that given the estimated $\\mu$, the prediction function for a new sample $x$ is given by:\n",
    "\n",
    "$$\n",
    "y = \\mathrm{sign}(\\sum_{i=1}^{n} \\mu_i y_i x_i^\\top x + b) \\enspace .\n",
    "$$\n",
    "\n",
    "The vector $w$ is therefore equal to $\\sum_{i=1}^{n} \\mu_i y_i x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "We have constructed the optimisation problem to find a function $f$ such that $y = f(x)$ where $f(x) = \\mathrm{sg}(w^\\top x + b)$. \n",
    "Moreover, we have derived previously that at the optimum:\n",
    "$$ w = \\sum_{i=1}^n \\mu_i y_i x_i $$\n",
    "It is then obvious that given the estimated $\\mu$, the prediction for a new sample $x$ is given by:\n",
    "$$ y = \\mathrm{sg}\\left( \\sum_{i=1}^n \\mu_i y_i x_i^\\top x + b \\right) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation of solver with intercept using cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file svm_project_utils.py contains the code to generate some toy data and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from svm_project_utils import plot_dataset, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300)\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following function qp allows to solve a quadratic problem of the form:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "  \\begin{array}{cll}\n",
    "  &\\min_{(x)} \\frac{1}{2}x^{\\top} H x - e^\\top x\n",
    "  \\\\\n",
    "   & \\textrm{s.c.}\\; A^\\top x = b, 0 \\leq x \\leq C.\n",
    "  \\end{array}\n",
    "  \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "\n",
    "def qp(H, e, A, b, C=np.inf, l=1e-8, verbose=True):\n",
    "    # Gram matrix\n",
    "    n = H.shape[0]\n",
    "    H = cvxopt.matrix(H)\n",
    "    A = cvxopt.matrix(y, (1, n))\n",
    "    e = cvxopt.matrix(-e)\n",
    "    b = cvxopt.matrix(0.0)\n",
    "    if C == np.inf:\n",
    "        G = cvxopt.matrix(np.diag(np.ones(n) * -1))\n",
    "        h = cvxopt.matrix(np.zeros(n))\n",
    "    else:\n",
    "        G = cvxopt.matrix(np.concatenate([np.diag(np.ones(n) * -1),\n",
    "                                         np.diag(np.ones(n))], axis=0))\n",
    "        h = cvxopt.matrix(np.concatenate([np.zeros(n), C * np.ones(n)]))\n",
    "\n",
    "    # Solve QP problem\n",
    "    cvxopt.solvers.options['show_progress'] = verbose\n",
    "    solution = cvxopt.solvers.qp(H, e, G, h, A, b)\n",
    " \n",
    "    # Lagrange multipliers\n",
    "    mu = np.ravel(solution['x'])\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel(X1, X2):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    K = np.empty((n1, n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = np.dot(X1[i], X2[j])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Q4: Modify the following cell to solve the SVM dual problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=0.7)\n",
    "plot_dataset(X, y)\n",
    "\n",
    "def svm_solver(K, y, C=np.inf):\n",
    "    y = y.reshape((-1, 1))\n",
    "    H = np.multiply(y.dot(y.T), K)\n",
    "    e = np.array([1.] * y.shape[0]).reshape((-1, 1))\n",
    "    A = y\n",
    "    b = 0.\n",
    "    mu = qp(H, e, A, b, C, l=1e-8, verbose=False)\n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "    return mu_support, idx_support\n",
    "\n",
    "K = kernel(X, X)\n",
    "\n",
    "# Uncomment the following lines when your svm_solver is completed:\n",
    "mu_support, idx_support = svm_solver(K, y)\n",
    "print(\"Number of support vectors: %s\" % idx_support.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q5: Compute w from mu and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.multiply(X[idx_support].T,y[idx_support]).dot(mu_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q6: Using complementary slackness, explain how to obtain $b$ from $\\mu$.\n",
    "\n",
    "HINT: Use the fact that for all support vectors for which $\\mu_i$ is non-zero one has $y_{i}(w^{t}x_{i}+b) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The KKT conditions ensure that $\\forall i$:\n",
    "- either $\\mu_i = 0$,\n",
    "- or $y_i(w^\\top x_i + b) = 1$\n",
    "\n",
    "To find $b$, it therefore suffices to pick any index $i$ in the support of $\\mu$, and deduce from the above that \n",
    "$$\\begin{align} \n",
    "b &= y_i - w^\\top x_i\\\\\n",
    "&= y_i - \\sum_{j=1}^n \\mu_j y_j x_j^\\top x_i\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_b(K, y, mu_support, idx_support):\n",
    "    y_support = y[idx_support]\n",
    "    K_support = K[idx_support][:, idx_support]\n",
    "    b = y_support[0] -  np.sum(mu_support * y_support * K_support[0])\n",
    "    return b\n",
    "\n",
    "b = compute_b(K, y, mu_support, idx_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q7: Verify that the constraints of the primal problem are satistified up to an acceptable numerical precision. You should verify that for all $i$ we have:\n",
    "\n",
    "$$\n",
    "y_{i}(w^{\\top}x_{i}+b) \\geq 1 - \\epsilon\n",
    "$$\n",
    "\n",
    "using for example $\\epsilon = 1e-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(X, y, w, b, precision = 1e-5):\n",
    "    return all(xi >= 1-precision for xi in (y * (X.dot(w) + b)))\n",
    "\n",
    "print(check(X, y, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your code by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=0.7)\n",
    "\n",
    "K = kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "def plot_classif(X, y, mu_support, idx_support, b, kernel=kernel):\n",
    "    # Calcul de la fonction de décision sur une grille\n",
    "    X1, X2 = np.mgrid[-4:4:0.1, -4:4:0.1]\n",
    "    na, nb = X1.shape\n",
    "    X_test = np.c_[np.reshape(X1, (na * nb, 1)),\n",
    "                   np.reshape(X2, (na * nb, 1))]\n",
    "\n",
    "    # Calcul des produits scalaires\n",
    "    X_support = X[idx_support]\n",
    "    G = kernel(X_test, X_support)\n",
    "    # Calcul de la fonction de décision\n",
    "    decision = G.dot(mu_support * y[idx_support]) + b\n",
    "\n",
    "    # Calcul du label prédit\n",
    "    y_pred = np.sign(decision)\n",
    "\n",
    "    # Affichage des lignes de niveau de la fonction de decision\n",
    "    plt.contourf(X1, X2, np.reshape(decision, (na, nb)), 20, cmap=plt.cm.gray)\n",
    "    cs = plt.contour(X1, X2, np.reshape(decision, (na,nb)), [-1, 0, 1], color='g', linewidth=2)\n",
    "    plt.clabel(cs, inline=1)\n",
    "    plt.plot(X[y == 1,0], X[y == 1, 1], 'or', linewidth=2)\n",
    "    plt.plot(X[y == -1,0], X[y == -1, 1], 'ob', linewidth=2)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now change the value of $\\sigma$ such that the problem is not linearily separable anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.5)\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "w = np.sum((mu_support * y[idx_support])[: , None] * X[idx_support], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q8: Check that contraints of the problem are now violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(check(X, y, w, b))\n",
    "plot_classif(X, y, mu_support, idx_support, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Non separable case with cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is very likely that the classes are not linearly separable.\n",
    "\n",
    "A very natural idea is to relax the constraints $y_{i}(w^\\top x_i + c) \\geq 1$.\n",
    "To do this, so called soft-margin SVM have been introduced using\n",
    "so called slack variables: $\\xi_{i}\\geq 0$. The problem becomes:\n",
    "\n",
    "$$\n",
    " y_{i}(w^\\top x_i + b) \\geq 1 - \\xi_i, \\; \\xi_i \\geq 0 \\enspace .\n",
    "$$\n",
    "\n",
    "Note that if $\\xi_i > 1$, the sample $x_{i}$ will be misclassified. To prevent\n",
    "this case to be too frequent, an idea is to minimize the sum of the $\\xi_{i}$.\n",
    "This leads to the following problem:\n",
    "\n",
    "$$\n",
    "(P_{s}):  \\left\\{\n",
    " \\begin{array}{ll}\n",
    " \\min_{(w,b,\\xi)} & \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i\n",
    " \\\\\n",
    " \\mathrm{s.t.} & y_{i}(w^{\\top}x_{i}+b) \\geq 1 - \\xi_i\\\\\n",
    " \\mathrm{and} & -\\xi_i \\leq 0\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$\n",
    "\n",
    "The constant $C$ controls the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q9: Justify that $(P_{s})$ is a convex problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "$(P_s)$ is the minimization of a convex function of ($w,\\xi$) under affine inequality constraints. It is therefore a convex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show that a dual problem of $(P_{s})$ reads:\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "\\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{\\top}\\mu-\\mu^{\\top}u\n",
    "\\\\\n",
    "\\mathrm{s.t.}& y^{\\top}\\mu = 0\n",
    "\\\\\n",
    "\\mathrm{et}& 0 \\leq \\mu \\leq C\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Introducing Lagrange multipliers $\\mu, \\alpha$ for the two types of inequality constraints, the Lagragian of $(P_s)$ is $\\mathcal{L}(w,b,\\xi,\\mu,\\alpha) =  \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i - \\sum_i \\mu_i [(w^\\top x_i + b) - (1-\\xi_i)] - \\sum_i \\alpha_i \\xi_i$\n",
    "\n",
    "Since $(P_s)$ is a convex problem, its KKT conditions are necessary and sufficient. In particular, at an optimum the gradient of its Lagragian vanishes, which yields \n",
    "$\\begin{align}\n",
    "\\nabla_w\\mathcal{L}(w,b,\\xi,\\mu,\\alpha) &= w - \\sum_i\\mu_i y_i x_i = 0\\\\\n",
    "\\frac{\\partial\\mathcal{L}(w,b,\\xi,\\mu,\\alpha)}{\\partial b} &= - \\sum_i\\mu_i y_i= 0\\\\\n",
    "\\nabla_\\xi\\mathcal{L}(w,b,\\xi,\\mu,\\alpha) &= C\\mathbb{1} - (\\mu + \\alpha) = 0\n",
    "\\end{align}$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$\\begin{align}\n",
    "&w = G^\\top \\mu\\\\\n",
    "&\\mu^\\top y = 0\\\\\n",
    "&Cu = (\\mu + \\alpha)\n",
    "\\end{align}$\n",
    "\n",
    "where $u$ is the vector of ones.\n",
    "\n",
    "The KKT conditions also give us that\n",
    "\n",
    "$\\begin{align}\n",
    "&\\forall i, \\mu_i[y_i(w^\\top x_i +b) - (1 - \\xi_i)] = 0\\\\\n",
    "&\\forall i, \\alpha_i \\mu_i = 0\n",
    "&\\forall i, \\mu_i \\geq 0, \\alpha_i \\geq 0, \\alpha_i + \\mu_i = 0\n",
    "\\end{align}$\n",
    "\n",
    "In particular, we can rewrite the Lagrangian at the optimum as\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(w,b,\\xi,\\mu,\\alpha) &=  \\frac{1}{2}\\mu^\\top GG^\\top \\mu + \\sum_i \\mu_i \\xi_i - \\sum_i \\mu_i [(w^\\top x_i + b) - (1-\\xi_i)]\\\\\n",
    "& = \\frac{1}{2}\\mu^\\top GG^\\top \\mu - \\sum_i \\mu_i [(w^\\top x_i + b) - 1]\\\\\n",
    "& = \\frac{1}{2}\\mu^\\top GG^\\top \\mu - \\mu^\\top GG^\\top \\mu + \\mu^\\top u\\\\\n",
    "&= - \\frac{1}{2}\\mu^\\top GG^\\top \\mu + \\mu^\\top u\n",
    "\\end{align}$\n",
    "\n",
    "Therefore, we can consider the dual problem:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\max_\\mu \\left\\lbrace -\\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } \\mu \\geq 0, \\alpha \\geq 0, Cu = \\alpha + \\mu \\text{ and } y^\\top \\mu = 0 \\end{cases}$$\n",
    "which is equivalent to:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\min_\\mu \\left\\lbrace \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } 0 \\leq \\mu \\leq C \\\\ \\text{and } y^\\top \\mu = 0 \\end{cases} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q10: Modify your code from Q4 to handle the non-separable case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.7)\n",
    "\n",
    "K = kernel(X, X)\n",
    "C = 1\n",
    "mu_support, idx_support = svm_solver(K, y, C=C)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q11: What is the influence of C on the number of support vectors? Justify this from an optimization stand point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.7)\n",
    "K = kernel(X, X)\n",
    "\n",
    "for C in np.logspace(-4, 2, 15):\n",
    "    mu_support, idx_support = svm_solver(K, y, C=C)\n",
    "    b = compute_b(K, y, mu_support, idx_support)\n",
    "    print(\"C = {:.4f} --> Number of support vectors: {}\".format(C, len(idx_support)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "A small value of $C$ implies a high number of support vectors ($\\simeq 300$ for $C = 10^{-4}$). Indeed, if $C$ is low, the regularization is strong, and the allowed margin is large. Moreover, we have more chances to have vectors on the margin hyperplanes if the margin is large, that is why the number of support vectors is higher with small values of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: non-linear case with kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another scenario is when the boundary between classes is not linear.\n",
    "\n",
    "To cope with this the idea is to use kernels.\n",
    "\n",
    "- Q12: Denoting by $K(x_i, x_j)$ the dot product between samples show that dual problem and the decision function f(x) can be reformulated just using calls to $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Since $XX^\\top_{ij} = \\lbrace x_i,x_j\\rbrace = K(x_i,x_j)$, we have that $GG^\\top_{ij} = y_iK(x_i,x_j)y_j$ and therefore $\\mu^\\top GG^\\top\\mu = \\sum_{ij} \\mu_i y_i K(x_i,x_j)y_j \\mu_j$.\n",
    "\n",
    "Likewise, the decision function $f(x) = \\mathrm{sg}(w^\\top x + b)$ can be rewritten as\n",
    "\n",
    "$\\begin{align}\n",
    "f(x) &= \\mathrm{sg}(\\mu^\\top G x + b)\\\\\n",
    "&= \\mathrm{sg}\\left( \\sum_{i=1}^n \\mu_i y_i x_i^\\top x + b \\right)\\\\\n",
    "&= \\mathrm{sg}\\left( \\sum_{i=1}^n \\mu_i y_i K(x_i,x) + b \\right)\n",
    "\\end{align}$\n",
    "\n",
    "Both the minimization problem and the decision function can thus be reformulated in terms of kernel $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the clowns dataset to evaluate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='clowns', n_points=200, sigma=0.7)\n",
    "\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q13: Update your kernel function so it computes the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "    K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|)\n",
    "$$\n",
    "\n",
    "where $\\gamma > 0$ is the kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma=3.):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    K = np.empty((n1, n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = np.exp(-gamma*np.linalg.norm(X1[i] - X2[j]))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the change above the follwing code should allow you to nicely separate the red from the blue dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='clowns', n_points=200, sigma=.7)\n",
    "\n",
    "K = rbf_kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y, C=1)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel=rbf_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear SVM without intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of the formuation of SVMs with the intercept term $b$ is that\n",
    "it leads to an annoying constraint in the dual, namely the $y^{t}\\mu = 0$.\n",
    "\n",
    "We will now see what we can do about it.\n",
    "\n",
    "Let's consider the problem\n",
    "\n",
    "$$\n",
    "(P'_{s}):  \\left\\{\n",
    " \\begin{array}{ll}\n",
    " \\min_{(w,\\xi)} & \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i\n",
    " \\\\\n",
    " \\mathrm{s.t.} & y_{i}(w^{\\top}x_{i}) \\geq 1 - \\xi_i\\\\\n",
    " \\mathrm{and} & -\\xi_i \\leq 0\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q14: Show that a dual problem of $(P'_{s})$ is given by:\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "\\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{\\top}\\mu-\\mu^{\\top} 1_n\n",
    "\\\\\n",
    "\\mathrm{s.t.}& 0 \\leq \\mu \\leq C\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Introducing Lagrange multipliers $\\mu, \\alpha$ for the two types of inequality constraints, the Lagragian of $(P_s)$ is $\\mathcal{L}(w,\\xi,\\mu,\\alpha) =  \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i - \\sum_i \\mu_i [(w^\\top x_i) - (1-\\xi_i)] - \\sum_i \\alpha_i \\xi_i$\n",
    "\n",
    "Since $(P_s)$ is a convex problem, its KKT conditions are necessary and sufficient. In particular, at an optimum the gradient of its Lagragian vanishes, which yields \n",
    "$\\begin{align}\n",
    "\\nabla_w\\mathcal{L}(w,\\xi,\\mu,\\alpha) &= w - \\sum_i\\mu_i y_i x_i = 0\\\\\n",
    "\\nabla_\\xi\\mathcal{L}(w,\\xi,\\mu,\\alpha) &= C\\mathbb{1} - (\\mu + \\alpha) = 0\n",
    "\\end{align}$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$\\begin{align}\n",
    "&w = G^\\top \\mu\\\\\n",
    "&C\\mathbb{1} = (\\mu + \\alpha)\n",
    "\\end{align}$\n",
    "\n",
    "The KKT conditions also give us that\n",
    "\n",
    "$\\begin{align}\n",
    "&\\forall i, \\mu_i[y_i(w^\\top x_i) - (1 - \\xi_i)] = 0\\\\\n",
    "&\\forall i, \\alpha_i \\mu_i = 0\\\\\n",
    "&\\forall i, \\mu_i \\geq 0, \\alpha_i \\geq 0, \\alpha_i + \\mu_i = C\n",
    "\\end{align}$\n",
    "\n",
    "In particular, we can rewrite the Lagrangian at the optimum as\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathcal{L}(w,\\xi,\\mu,\\alpha) &=  \\frac{1}{2}\\mu^\\top GG^\\top \\mu + \\sum_i \\mu_i \\xi_i - \\sum_i \\mu_i [(w^\\top x_i) - (1-\\xi_i)]\\\\\n",
    "& = \\frac{1}{2}\\mu^\\top GG^\\top \\mu - \\sum_i \\mu_i [(w^\\top x_i) - 1]\\\\\n",
    "& = \\frac{1}{2}\\mu^\\top GG^\\top \\mu - \\mu^\\top GG^\\top \\mu + \\mu^\\top u\\\\\n",
    "&= - \\frac{1}{2}\\mu^\\top GG^\\top \\mu + \\mu^\\top u\n",
    "\\end{align}$\n",
    "\n",
    "Therefore, we can consider the dual problem:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\max_\\mu \\left\\lbrace -\\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } \\mu \\geq 0, \\alpha \\geq 0, C\\mathbb{1} = \\alpha + \\mu \\end{cases}$$\n",
    "which is equivalent to:\n",
    "$$ (\\mathcal{D}): \\begin{cases}\\min_\\mu \\left\\lbrace \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu^\\top u \\right\\rbrace \\\\ \\text{s.t. } 0 \\leq \\mu \\leq C\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q15: Rewrite the dual in the form:\n",
    "\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}): \\min_{\\mu} f(\\mu) + g(\\mu) .\n",
    "$$\n",
    "\n",
    "where $f$ is here a smooth function of $\\mu$ with L-Liptschitz gradient and $g$ is a non-smooth function that is separable, namely:\n",
    "\n",
    "$$\n",
    "g(\\mu) = \\sum_{i=1}^n g_i(\\mu_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The dual can be rewritten as $min_\\mu \\frac{1}{2}\\mu^\\top GG^\\top\\mu - \\mu^\\top u + \\mathbb{I}_{[0,C]^n}(\\mu)$ where:\n",
    "$$\\mathbb{I}_{[0,C]^n}(\\mu) = \n",
    "\\begin{cases} \n",
    "    +\\infty \\; \\text{if } \\exists i, \\mu_i \\notin [0,C] \\\\ \n",
    "    0 \\; \\text{otherwise} \n",
    "\\end{cases}$$\n",
    "\n",
    "Clearly, $f(\\mu) := \\frac{1}{2}\\mu^\\top GG^\\top\\mu - \\mu^\\top \\mathbb{1}$ and $g(\\mu) := \\sum_i (\\mathbb{I}_{[0,C]}(\\mu_i))$ meet the requirements and are such that \n",
    "$$(\\mathcal{D}): \\min_{\\mu} \\lbrace f(\\mu) + g(\\mu) \\rbrace$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual in the later form can be readily optimized using the methods that you have been studying in this class:\n",
    "\n",
    "- Proximal gradient method with and without acceleration\n",
    "- L-BFGS-B\n",
    "- Coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q16: Implement:\n",
    "\n",
    "   - your own version of proximal gradient with and without acceleration\n",
    "   - your own version of coordinate descent\n",
    "   - an L-BFGS-B solver using `scipy.optimize.fmin_l_bfgs_b`\n",
    "\n",
    "Note: We restrict ourselves to linear kernel here.\n",
    "\n",
    "Note: To handle separating hyperplanes which do not pass throw zero (due to abscence of intercept)\n",
    "you will add a column of ones to X. You can use something like this:\n",
    "\n",
    "`X = np.concatenate((X, np.ones((len(X), 1))), axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will test your implementations on the Gaussian blobs and evaluate the performance of your implementations in terms of computation time on problems where the matrix $G G^\\top$ can fit in memory.\n",
    "\n",
    "You should reuse as much as possible the convergence evaluation code that you used during the labs.\n",
    "\n",
    "For a coordinate descent method to be fast you need to have smart updates. You're expected to\n",
    "come up with these smart updates in the problem at hand.\n",
    "\n",
    "BONUS : With a smart implementation of the coordinate descent you should be able to scale the optimization to tens of thousands of samples ie cases where $G G^\\top$ does not fit in memory anymore.\n",
    "\n",
    "**IMPORTANT : This question Q16 is the most important and will constitute half of the final grade on the project !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A method from TP3 for evaluating the performance of the minimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inspector(loss_fun, verbose=False):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\"\"\"\n",
    "    objectives = []\n",
    "    # errors = []\n",
    "    times = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(xk):\n",
    "        obj = loss_fun(xk)# - f_min\n",
    "        # err = norm(xk - x_min)\n",
    "        objectives.append(obj)\n",
    "        # errors.append(err)\n",
    "        tim = time.clock() - epoch\n",
    "        times.append(tim)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"time\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8), (\"%.2e\" % tim).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.obj = objectives\n",
    "    # inspector_cl.err = errors\n",
    "    inspector_cl.times = times\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fun_gen(K, y):\n",
    "    GG_ = np.multiply(np.multiply(y, K).T, y)\n",
    "    def aux(mu):\n",
    "        return 1/2 * mu.T.dot(GG_.dot(mu)) - np.sum(mu)\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla proximal gradient descent (ISTA)\n",
    "\n",
    "$x_{k+1} = \\mathrm{prox}_g\\lbrace x_k - 1/L\\nabla f(x_k)\\rbrace$\n",
    "\n",
    "where $\\mathrm{prox}_g(x) = \\mathrm{Proj}_{[O,C]^n}(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "def grad(GG, mu):\n",
    "    return GG.dot(mu) - 1\n",
    "\n",
    "def prox(x, C):\n",
    "    return np.maximum(0, np.minimum(x,C))\n",
    "\n",
    "def lipschitz_constant(K, y):\n",
    "    \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "    return np.max(svd(np.multiply(np.multiply(y, K).T,y), full_matrices=False)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, y = datasets(name='clowns', n_points=200, sigma=0.7)\n",
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.7)\n",
    "X_ = np.concatenate((X, np.ones((len(X), 1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ista_svm(K, y, grad, prox, C, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"ISTA algorithm.\"\"\"\n",
    "\n",
    "    GG = np.multiply(np.multiply(y, K).T, y)\n",
    "    \n",
    "    mu = np.zeros(K.shape[0])\n",
    "    for _ in range(n_iter):\n",
    "        mu = prox(mu - step * grad(GG, mu), C)\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback: \n",
    "            callback(mu)\n",
    "    \n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "    return mu_support, idx_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test ISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X_, X_)\n",
    "step = 1./lipschitz_constant(K, y)\n",
    "ista_inspector = inspector(loss_fun_gen(K, y), verbose=True)\n",
    "epoch = time.clock()\n",
    "n_iter = 500\n",
    "mu_support, idx_support = ista_svm(K, y, grad, prox, C=1., n_iter=n_iter, step=step, callback=ista_inspector)\n",
    "b = 0\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel = kernel)\n",
    "#plt.plot(ista_inspector.times, ista_inspector.obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerated proximal gradient descent (FISTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fista_svm(K, y, grad, prox, C, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"FISTA algorithm.\"\"\"\n",
    "    \n",
    "    GG = np.multiply(np.multiply(y, K).T, y)\n",
    "    \n",
    "    mu = np.zeros(K.shape[0])\n",
    "    z = np.zeros(K.shape[0])\n",
    "    t = 1.\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        mu_new = prox(z - step * grad(GG, z), C)\n",
    "        t_new = (1 + np.sqrt(1 + 4*t*t)) / 2\n",
    "        z_new = mu_new + (t - 1) / t_new * (mu_new - mu)\n",
    "        mu, z, t = mu_new, z_new, t_new\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback: \n",
    "            callback(mu)\n",
    "    \n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "    return mu_support, idx_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X_, X_)\n",
    "step = 1./lipschitz_constant(K,y)\n",
    "fista_inspector = inspector(loss_fun_gen(K, y), verbose=True)\n",
    "epoch = time.clock()\n",
    "n_iter = 500\n",
    "mu_support, idx_support = fista_svm(K, y, grad, prox, C = 1., n_iter=n_iter, step=step, callback=fista_inspector)\n",
    "b = 0\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel = kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cd_svm(K, y, C, n_iter=50, callback=None):\n",
    "    \"\"\"Proximal gradient descent algorithm\"\"\"\n",
    "    \n",
    "    GG = np.multiply(np.multiply(y, K).T, y)\n",
    "    n_features = K.shape[0]\n",
    "        \n",
    "    mu = np.zeros(K.shape[0])\n",
    "    L = np.array([np.sqrt(np.dot(GG[:, i], GG[:, i])) for i in range(GG.shape[1])]) # we pre-compute Gi.T * Gi = Lipschitz constants\n",
    "    \n",
    "    for k in range(n_iter + 1):\n",
    "        i = k % n_features # cyclic feature selection\n",
    "        \n",
    "        mu[i] -= (GG[:, i].T.dot(mu) - 1) / L[i]\n",
    "        mu[i] = prox(mu[i], C)\n",
    "        \n",
    "        # Update metrics after each iteration.\n",
    "        if callback: \n",
    "            callback(mu)\n",
    "        \n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "        \n",
    "    return mu_support, idx_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X_, X_)\n",
    "\n",
    "cd_inspector = inspector(loss_fun_gen(K, y), verbose=True)\n",
    "epoch = time.clock()\n",
    "n_iter = 1000\n",
    "mu_support, idx_support = cd_svm(K, y, C=1., n_iter=n_iter, callback=cd_inspector)\n",
    "b = 0\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel = kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-BFGS-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def l_bfgs_b_svm(K, y, C, callback=None):\n",
    "    \"\"\"L-BFGS-B algorithm\"\"\"\n",
    "    \n",
    "    GG = np.multiply(np.multiply(y, K).T, y)\n",
    "    \n",
    "    fun = lambda x: 1/2 * x.dot(GG.dot(x)) - np.sum(x)\n",
    "    mu0 = np.zeros(K.shape[0])\n",
    "    fungrad = lambda x: grad(GG, x)\n",
    "    bounds = [(0, C)] * K.shape[0]\n",
    "            \n",
    "    mu, _, infos = fmin_l_bfgs_b(fun, mu0, fprime=fungrad, bounds=bounds, callback=callback)\n",
    "\n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "    \n",
    "    print(\"Converged in %d iterations\" % infos['nit'])\n",
    "        \n",
    "    return mu_support, idx_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X_, X_)\n",
    "\n",
    "lbfgs_inspector = inspector(loss_fun_gen(K, y), verbose=True)\n",
    "epoch = time.clock()\n",
    "n_iter = 1000\n",
    "mu_support, idx_support = l_bfgs_b_svm(K, y, C=1., callback=lbfgs_inspector)\n",
    "b = 0\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel = kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ista_inspector.times, ista_inspector.obj, label=\"ISTA\")\n",
    "plt.plot(fista_inspector.times, fista_inspector.obj, label=\"FISTA\")\n",
    "plt.plot(cd_inspector.times, cd_inspector.obj, label=\"CD\")\n",
    "plt.plot(lbfgs_inspector.times, lbfgs_inspector.obj, label=\"L-BFGS-B\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus: Coordinate Descent when the Kernel Matrix does not fit in memory\n",
    "\n",
    "We start by noting that since $\\mu$ is a sparse vector depending on a restricted number of support vector, it is not necessary to compute the kernel matrix for those vectors that are not in the support. Therefore, a memory-efficient approach is to iteratively optimize w.r.t a small (compared to the whole dataset) working set of support vectors, and then reassess this working set.\n",
    "\n",
    "We follow some guidelines given in \n",
    "\n",
    ">Léon Bottou and Chih-Jen Lin: Support Vector Machine Solvers, in Large Scale Kernel Machines, Léon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston editors, 1–28, MIT Press, Cambridge, MA., 2007.\n",
    "\n",
    "although we use a more naive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memory_cd_svm(X, y, C, n_iter=10, n_inner = 50, callback=None):\n",
    "    \"\"\"Memory-efficieng proximal coordinate descent algorithm\"\"\"\n",
    "    \n",
    "    X_ = np.concatenate((X, np.ones((len(X), 1))), axis=1)\n",
    "    mu = np.zeros(X.shape[0])\n",
    "    \n",
    "    working_idx = np.random.choice(len(mu), 100, replace = False)\n",
    "    \n",
    "    GG = np.multiply(np.multiply(y[working_idx], kernel(X_[working_idx],X_[working_idx])).T, y[working_idx])\n",
    "    L = np.array([np.sqrt(np.dot(GG[:, i], GG[:, i])) for i in range(GG.shape[1])]) # we pre-compute Gi.T * Gi = Lipschitz constants\n",
    "    n_features = len(working_idx)\n",
    "    \n",
    "    for _ in range(n_iter +1):\n",
    "        \n",
    "        not_working_idx = [i for i in range(len(mu)) if i not in working_idx.tolist()]\n",
    "        \n",
    "        g = 1 - y[working_idx]*np.sum(np.multiply(kernel(X_[working_idx],X_[not_working_idx]),\\\n",
    "                                            y[not_working_idx] * mu[not_working_idx]), axis = 1)\n",
    "        \n",
    "        for k in range(n_inner + 1):\n",
    "            i = k % n_features # cyclic feature selection\n",
    "        \n",
    "            mu[working_idx[i]] -= (GG[:, i].T.dot(working_idx) - g[i]) / L[i]\n",
    "            mu[working_idx[i]] = prox(mu[working_idx[i]], C)\n",
    "        \n",
    "            # Update metrics after each iteration.\n",
    "            if callback: \n",
    "                callback(mu)\n",
    "        \n",
    "        \n",
    "        idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "        mu_support = mu[idx_support]\n",
    "        \n",
    "        # Calcul des produits scalaires\n",
    "        X_support = X[idx_support]\n",
    "        working_candidates = np.random.choice(len(mu), size = int(0.1*len(mu)), replace = False)\n",
    "        G = kernel(X[working_candidates], X_support)\n",
    "        # Calcul de la fonction de décision\n",
    "        decision = G.dot(mu_support * y[idx_support])\n",
    "\n",
    "        # Calcul du label prédit et du working set\n",
    "        y_pred = np.sign(decision)\n",
    "        \n",
    "        working_idx = np.where(y[working_candidates]*decision < C)[0]\n",
    "        del G\n",
    "    \n",
    "        # Compute the restricted kernel matrix and loop again\n",
    "        GG = np.multiply(np.multiply(y[working_idx], kernel(X_[working_idx],X_[working_idx])).T, y[working_idx])\n",
    "        L = np.array([np.sqrt(np.dot(GG[:, i], GG[:, i])) for i in range(GG.shape[1])]) # we pre-compute Gi.T * Gi = Lipschitz constants\n",
    "        n_features = len(working_idx)\n",
    "        \n",
    "    return mu_support, idx_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_support, idx_support = memory_cd_svm(X, y, C=1., n_iter=20, n_inner = 50, callback=None)\n",
    "b = 0\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel = kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
